{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Relational Graph Convolutional Network Tutorial\n",
    "================================================\n",
    "\n",
    "**Author:** Lingfan Yu, Mufei Li, Zheng Zhang\n",
    "\n",
    "The vanilla Graph Convolutional Network (GCN)\n",
    "(`paper <https://arxiv.org/pdf/1609.02907.pdf>`_,\n",
    "`DGL tutorial <http://doc.dgl.ai/tutorials/index.html>`_) exploits\n",
    "structural information of the dataset (i.e. the graph connectivity) to\n",
    "improve the extraction of node representations. Graph edges are left as\n",
    "untyped.\n",
    "\n",
    "A knowledge graph is made up by a collection of triples of the form\n",
    "(subject, relation, object). Edges thus encode important information and\n",
    "have their own embeddings to be learned. Furthermore, there may exist\n",
    "multiple edges among any given pair.\n",
    "\n",
    "A recent model Relational-GCN (R-GCN) from the paper\n",
    "`Modeling Relational Data with Graph Convolutional\n",
    "Networks <https://arxiv.org/pdf/1703.06103.pdf>`_ is one effort to\n",
    "generalize GCN to handle different relations between entities in knowledge\n",
    "base. This tutorial shows how to implement R-GCN with DGL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-GCN: a brief introduction\n",
    "---------------------------\n",
    "In *statistical relational learning* (SRL), there are two fundamental\n",
    "tasks:\n",
    "\n",
    "- **Entity classification**, i.e., assign types and categorical\n",
    "  properties to entities.\n",
    "- **Link prediction**, i.e., recover missing triples.\n",
    "\n",
    "In both cases, missing information are expected to be recovered from\n",
    "neighborhood structure of the graph. Here is the example from the R-GCN\n",
    "paper:\n",
    "\n",
    "\"Knowing that Mikhail Baryshnikov was educated at the Vaganova Academy\n",
    "implies both that Mikhail Baryshnikov should have the label person, and\n",
    "that the triple (Mikhail Baryshnikov, lived in, Russia) must belong to the\n",
    "knowledge graph.\"\n",
    "\n",
    "R-GCN solves these two problems using a common graph convolutional network\n",
    "extended with multi-edge encoding to compute embedding of the entities, but\n",
    "with different downstream processing:\n",
    "\n",
    "- Entity classification is done by attaching a softmax classifier at the\n",
    "  final embedding of an entity (node). Training is through loss of standard\n",
    "  cross-entropy.\n",
    "- Link prediction is done by reconstructing an edge with an autoencoder\n",
    "  architecture, using a parameterized score function. Training uses negative\n",
    "  sampling.\n",
    "\n",
    "This tutorial will focus on the first task to show how to generate entity\n",
    "representation. `Complete\n",
    "code <https://github.com/dmlc/dgl/tree/rgcn/examples/pytorch/rgcn>`_\n",
    "for both tasks can be found in DGL's github repository.\n",
    "\n",
    "Key ideas of R-GCN\n",
    "-------------------\n",
    "Recall that in GCN, the hidden representation for each node $i$ at\n",
    "$(l+1)^{th}$ layer is computed by:\n",
    "\n",
    "\\begin{align}h_i^{l+1} = \\sigma\\left(\\sum_{j\\in N_i}\\frac{1}{c_i} W^{(l)} h_j^{(l)}\\right)~~~~~~~~~~(1)\\\\\\end{align}\n",
    "\n",
    "where $c_i$ is a normalization constant.\n",
    "\n",
    "The key difference between R-GCN and GCN is that in R-GCN, edges can\n",
    "represent different relations. In GCN, weight $W^{(l)}$ in equation\n",
    "$(1)$ is shared by all edges in layer $l$. In contrast, in\n",
    "R-GCN, different edge types use different weights and only edges of the\n",
    "same relation type $r$ are associated with the same projection weight\n",
    "$W_r^{(l)}$.\n",
    "\n",
    "So the hidden representation of entities in $(l+1)^{th}$ layer in\n",
    "R-GCN can be formulated as the following equation:\n",
    "\n",
    "\\begin{align}h_i^{l+1} = \\sigma\\left(W_0^{(l)}h_i^{(l)}+\\sum_{r\\in R}\\sum_{j\\in N_i^r}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}\\right)~~~~~~~~~~(2)\\\\\\end{align}\n",
    "\n",
    "where $N_i^r$ denotes the set of neighbor indices of node $i$\n",
    "under relation $r\\in R$ and $c_{i,r}$ is a normalization\n",
    "constant. In entity classification, the R-GCN paper uses\n",
    "$c_{i,r}=|N_i^r|$.\n",
    "\n",
    "The problem of applying the above equation directly is rapid growth of\n",
    "number of parameters, especially with highly multi-relational data. In\n",
    "order to reduce model parameter size and prevent overfitting, the original\n",
    "paper proposes to use basis decomposition:\n",
    "\n",
    "\\begin{align}W_r^{(l)}=\\sum\\limits_{b=1}^B a_{rb}^{(l)}V_b^{(l)}~~~~~~~~~~(3)\\\\\\end{align}\n",
    "\n",
    "Therefore, the weight $W_r^{(l)}$ is a linear combination of basis\n",
    "transformation $V_b^{(l)}$ with coefficients $a_{rb}^{(l)}$.\n",
    "The number of bases $B$ is much smaller than the number of relations\n",
    "in the knowledge base.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Another weight regularization, block-decomposition, is implemented in\n",
    "   the `link prediction <link-prediction_>`_.</p></div>\n",
    "\n",
    "Implement R-GCN in DGL\n",
    "----------------------\n",
    "\n",
    "An R-GCN model is composed of several R-GCN layers. The first R-GCN layer\n",
    "also serves as input layer and takes in features (e.g. description texts)\n",
    "associated with node entity and project to hidden space. In this tutorial,\n",
    "we only use entity id as entity feature.\n",
    "\n",
    "R-GCN Layers\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "For each node, an R-GCN layer performs the following steps:\n",
    "\n",
    "- Compute outgoing message using node representation and weight matrix\n",
    "  associated with the edge type (message function)\n",
    "- Aggregate incoming messages and generate new node representations (reduce\n",
    "  and apply function)\n",
    "\n",
    "The following is the definition of an R-GCN hidden layer.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Each relation type is associated with a different weight. Therefore,\n",
    "   the full weight matrix has three dimensions: relation, input_feature,\n",
    "   output_feature.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "import dgl\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
    "                 activation=None, is_input_layer=False):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # weight bases in equation (3)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
    "                                                self.out_feat))\n",
    "        #self.weight_nodes = nn.Parameter(torch.Tensor(self.in_feat, self.))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(self.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.bias,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, g):\n",
    "        print(\"Starting Forwarding*************** \")\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases (equation (3))\n",
    "            \n",
    "            print(weight)\n",
    "            print(weight.shape)\n",
    "            \n",
    "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
    "            print(weight)\n",
    "            print(weight.shape)\n",
    "            \n",
    "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,self.in_feat, self.out_feat)\n",
    "            print(weight)\n",
    "            print(weight.shape)\n",
    "                                                        \n",
    "        else:\n",
    "            weight = self.weight\n",
    "            \n",
    "            print(weight)\n",
    "            print(weight.shape)\n",
    "            \n",
    "    \n",
    "        if self.is_input_layer:\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                print(\"Input_Layer\")\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                print(embed)\n",
    "                index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                print(edges.data['rel_type'])\n",
    "                print(index)\n",
    "                print(edges.data['norm'])\n",
    "                print(embed[index]* edges.data['norm'])\n",
    "                return {'msg': embed[index] * edges.data['norm']}\n",
    "        else:\n",
    "            def message_func(edges):\n",
    "                \n",
    "                w = weight[edges.data['rel_type']]\n",
    "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data['norm']\n",
    "                print(\"Message shape and message\")\n",
    "                print(msg.shape)\n",
    "                print(msg)\n",
    "                return {'msg': msg }\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            \n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {'h': h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)\n",
    "        print(\"Ending Forwarding*************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define full R-GCN model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        \n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        \n",
    "        # hidden to hidden\n",
    "        \n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        \n",
    "        # hidden to output\n",
    "        \n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node This needs to Be modified As per our dataset\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(self.num_nodes, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu, is_input_layer=True)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.out_dim, self.num_rels, self.num_bases,\n",
    "                         activation=partial(F.softmax, dim=1))\n",
    "\n",
    "    def forward(self, g):\n",
    "        #Dont need to initialise in our case as we have the features\n",
    "        if self.features is not None:\n",
    "            g.ndata['id'] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset\n",
    "~~~~~~~~~~~~~~~~\n",
    "In this tutorial, we use AIFB dataset from R-GCN paper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n",
      "91\n",
      "[ 289 6747 4779 6340 6480 1232 4987 7929 5247 2100  839 2776 2306 3599\n",
      " 6538 5914 4463 7005  781  590 1962 6949 4619 3124 3249 8198 6817 6268\n",
      " 3655  941 6589 2572 4126 1153 2225 7710 3920 2666 8166 2558 1650 4493\n",
      " 6155 4488 1509 7409 4669 2011 1919 3616   76 1626 1069 3871  448 3962\n",
      " 5638 1177 1048 6810 3852 5763 5080  652 5791 4193 5428 7877 3236 5107\n",
      " 3613 3429 2246  712 2170 3795 5698 6995 4703 2368 1894  190 2390 6805\n",
      " 1535 2512 6038 1415  390 1061 2842 2051 3527 6724  857 2280  811 7031\n",
      "  737  552 5000 7659 1158 7765 7046 6036  876 5412 3596 1347 7850 4907]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        ...,\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [1.0000]])\n",
      "tensor([ 0, 61,  0,  ...,  1,  6,  0])\n",
      "torch.Size([65439])\n"
     ]
    }
   ],
   "source": [
    "# load graph data\n",
    "from dgl.contrib.data import load_data\n",
    "import numpy as np\n",
    "import dgl\n",
    "data = load_data(dataset='aifb')\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "print(num_rels)\n",
    "#Some modifications for batched graph adding training samples for the second graph as well\n",
    "\n",
    "\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "print(train_idx)\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "print(labels)\n",
    "labels = torch.from_numpy(labels).view(-1)\n",
    "print(edge_norm)\n",
    "print(edge_type)\n",
    "print(edge_type.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph and model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from dgl import DGLGraph\n",
    "#nx.draw(g.to_networkx(), with_labels=True)\n",
    "plt.show()\n",
    "# configurations\n",
    "n_hidden = 16 # number of hidden units\n",
    "n_bases = -1 # use number of relations as number of bases\n",
    "n_hidden_layers = 0 # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 25 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient\n",
    "\n",
    "# create graph instance\n",
    "g = DGLGraph()\n",
    "g.add_nodes(num_nodes)\n",
    "g.add_edges(data.edge_src, data.edge_dst)\n",
    "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "\n",
    "# create model\n",
    "model = Model(len(g),\n",
    "              n_hidden,\n",
    "              num_classes,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Starting Forwarding*************** \n",
      "Parameter containing:\n",
      "tensor([[[ 7.8686e-04,  4.6842e-03, -7.9807e-03,  ...,  6.6052e-04,\n",
      "          -5.4025e-03,  8.8233e-03],\n",
      "         [ 3.6271e-03,  9.1260e-03,  3.8814e-03,  ..., -4.9560e-03,\n",
      "           5.5942e-03, -4.3889e-03],\n",
      "         [-4.1957e-03, -2.5508e-03,  8.3233e-03,  ..., -5.7547e-04,\n",
      "           7.2031e-03,  1.2048e-03],\n",
      "         ...,\n",
      "         [-6.7929e-03, -6.5432e-03, -4.0091e-03,  ...,  1.8005e-03,\n",
      "          -5.8873e-03, -4.7668e-03],\n",
      "         [ 6.1361e-03, -7.8966e-03,  4.3115e-03,  ..., -4.1180e-03,\n",
      "          -6.1953e-03,  6.3274e-03],\n",
      "         [ 1.8656e-03,  5.0603e-03, -1.6978e-03,  ...,  4.8858e-03,\n",
      "           6.1309e-03,  2.6589e-03]],\n",
      "\n",
      "        [[ 7.1446e-04, -9.3191e-03, -9.0026e-03,  ...,  4.7601e-04,\n",
      "           5.7390e-03, -9.2161e-03],\n",
      "         [ 2.8392e-03, -7.0548e-03,  3.7334e-03,  ..., -6.8608e-04,\n",
      "           1.6253e-03,  1.5556e-03],\n",
      "         [ 6.1136e-03,  3.0482e-03,  8.0929e-03,  ...,  5.9130e-03,\n",
      "           1.4943e-04,  2.1522e-03],\n",
      "         ...,\n",
      "         [-3.9885e-03, -7.0561e-03,  2.8760e-03,  ..., -3.3390e-03,\n",
      "           8.6986e-03, -6.4588e-03],\n",
      "         [ 9.0861e-03,  2.2059e-03,  2.9497e-03,  ...,  3.7920e-03,\n",
      "          -1.0062e-03,  2.5105e-03],\n",
      "         [ 8.3837e-03,  8.4525e-03,  9.3924e-03,  ...,  4.1807e-03,\n",
      "           5.4676e-03,  4.1303e-03]],\n",
      "\n",
      "        [[ 7.1670e-03, -3.4084e-03, -3.0325e-04,  ..., -1.9694e-03,\n",
      "          -1.7195e-03,  4.7940e-03],\n",
      "         [ 7.1139e-03,  2.8621e-03, -3.7059e-03,  ...,  5.1662e-03,\n",
      "          -8.4752e-03,  2.9844e-03],\n",
      "         [-6.6006e-03,  7.9047e-03, -2.6224e-03,  ..., -1.3513e-03,\n",
      "           8.5914e-03, -2.5986e-03],\n",
      "         ...,\n",
      "         [ 8.2603e-03, -6.0706e-03, -8.0144e-03,  ..., -5.2428e-03,\n",
      "          -4.2289e-03, -8.4165e-03],\n",
      "         [ 4.0286e-03,  5.6349e-03,  5.0453e-03,  ...,  6.2600e-03,\n",
      "          -4.0421e-03,  7.6352e-03],\n",
      "         [ 1.1463e-03,  4.8966e-03,  5.0169e-03,  ...,  2.0910e-03,\n",
      "           3.2084e-03,  4.7889e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.9485e-04, -4.3391e-04, -9.4115e-03,  ...,  1.1041e-03,\n",
      "          -4.4672e-03,  8.9129e-03],\n",
      "         [ 7.1388e-03, -7.4392e-03,  1.9357e-03,  ..., -2.9856e-03,\n",
      "           8.0257e-03, -7.6751e-03],\n",
      "         [-5.7825e-03, -5.3195e-04,  2.9393e-03,  ...,  3.1372e-03,\n",
      "          -3.9811e-03, -8.0941e-03],\n",
      "         ...,\n",
      "         [-8.3711e-03, -7.4023e-03, -6.9152e-03,  ...,  6.8812e-03,\n",
      "           2.6779e-03, -8.2343e-05],\n",
      "         [ 3.1290e-03, -4.7483e-03, -5.0285e-03,  ..., -9.3979e-03,\n",
      "           8.2246e-03,  4.1062e-03],\n",
      "         [-4.4787e-03,  8.8105e-03,  6.1090e-03,  ...,  3.2495e-03,\n",
      "          -9.0334e-03, -5.3156e-03]],\n",
      "\n",
      "        [[-7.5585e-03,  7.5020e-03,  7.0007e-03,  ...,  2.9783e-03,\n",
      "           8.7768e-03,  4.4444e-03],\n",
      "         [-8.1667e-03, -9.2836e-04,  2.1755e-03,  ..., -6.4540e-03,\n",
      "          -5.4885e-03, -4.7588e-03],\n",
      "         [ 8.6028e-03, -7.6841e-03,  4.0604e-03,  ..., -8.4020e-03,\n",
      "          -2.4139e-03,  2.9581e-03],\n",
      "         ...,\n",
      "         [-6.9050e-03,  2.7142e-03, -8.9502e-03,  ..., -2.7287e-04,\n",
      "           3.1951e-03, -7.3126e-03],\n",
      "         [-5.4069e-03,  8.3827e-03, -8.7633e-03,  ..., -1.8548e-03,\n",
      "          -1.5718e-03,  8.4109e-03],\n",
      "         [ 5.0499e-03, -6.3041e-03,  3.0359e-03,  ..., -3.9453e-03,\n",
      "          -6.7751e-03,  4.2488e-03]],\n",
      "\n",
      "        [[-3.2346e-03, -8.9703e-03, -3.5989e-03,  ...,  2.7764e-03,\n",
      "          -7.8340e-03, -5.2776e-03],\n",
      "         [-4.7950e-03,  3.1287e-03, -5.4543e-03,  ..., -4.8350e-03,\n",
      "          -7.2654e-03,  9.4158e-03],\n",
      "         [ 7.1342e-03,  8.7523e-03, -2.7041e-03,  ..., -8.7986e-03,\n",
      "           5.6627e-03, -8.4385e-03],\n",
      "         ...,\n",
      "         [-5.1577e-04,  5.3338e-03, -3.5760e-04,  ...,  1.9887e-03,\n",
      "           1.5383e-03,  4.7865e-03],\n",
      "         [-1.4288e-03, -6.1320e-03,  4.2810e-03,  ...,  6.6861e-03,\n",
      "          -3.9521e-03,  4.6477e-03],\n",
      "         [ 1.8080e-03,  5.3730e-03, -5.1611e-03,  ...,  6.7994e-03,\n",
      "          -6.5675e-03,  7.6481e-03]]], requires_grad=True)\n",
      "torch.Size([91, 8285, 16])\n",
      "Input_Layer\n",
      "tensor([[ 0.0008,  0.0047, -0.0080,  ...,  0.0007, -0.0054,  0.0088],\n",
      "        [ 0.0036,  0.0091,  0.0039,  ..., -0.0050,  0.0056, -0.0044],\n",
      "        [-0.0042, -0.0026,  0.0083,  ..., -0.0006,  0.0072,  0.0012],\n",
      "        ...,\n",
      "        [-0.0005,  0.0053, -0.0004,  ...,  0.0020,  0.0015,  0.0048],\n",
      "        [-0.0014, -0.0061,  0.0043,  ...,  0.0067, -0.0040,  0.0046],\n",
      "        [ 0.0018,  0.0054, -0.0052,  ...,  0.0068, -0.0066,  0.0076]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([ 0, 61,  0,  ...,  1,  6,  0])\n",
      "tensor([     0, 509503,      1,  ...,  15821,  57246,   8284])\n",
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        ...,\n",
      "        [0.1000],\n",
      "        [0.1000],\n",
      "        [1.0000]])\n",
      "tensor([[ 7.8686e-04,  4.6842e-03, -7.9807e-03,  ...,  6.6052e-04,\n",
      "         -5.4025e-03,  8.8233e-03],\n",
      "        [-7.0958e-03,  6.1397e-03, -4.2036e-03,  ...,  4.7864e-03,\n",
      "          4.2232e-03, -2.1674e-03],\n",
      "        [ 3.6271e-03,  9.1260e-03,  3.8814e-03,  ..., -4.9560e-03,\n",
      "          5.5942e-03, -4.3889e-03],\n",
      "        ...,\n",
      "        [-5.7518e-05,  4.0528e-04,  3.8861e-04,  ...,  2.6251e-04,\n",
      "         -5.5754e-04,  4.2622e-04],\n",
      "        [-9.3450e-04,  7.5075e-04, -8.5081e-04,  ...,  3.5835e-04,\n",
      "          9.4381e-04, -5.6478e-05],\n",
      "        [ 1.8656e-03,  5.0603e-03, -1.6978e-03,  ...,  4.8858e-03,\n",
      "          6.1309e-03,  2.6589e-03]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "apply_func() missing 1 required positional argument: 'edges'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1f25a96b004e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-de0216ffbcb7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mll/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-803fac5ac18a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'msg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ending Forwarding*************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mll/lib/python3.6/site-packages/dgl/graph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func)\u001b[0m\n\u001b[1;32m   2549\u001b[0m                                           \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                                           apply_func=apply_node_func)\n\u001b[0;32m-> 2551\u001b[0;31m             \u001b[0mRuntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m     def prop_nodes(self,\n",
      "\u001b[0;32m~/.conda/envs/mll/lib/python3.6/site-packages/dgl/runtime/runtime.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(prog)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m#prog.pprint_exe(exe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/mll/lib/python3.6/site-packages/dgl/runtime/ir/executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mnode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdmail\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mudf_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mmail_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdmail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mll/lib/python3.6/site-packages/dgl/runtime/scheduler.py\u001b[0m in \u001b[0;36m_afunc_wrapper\u001b[0;34m(node_data)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_afunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0mnbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNodeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mapply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mafunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFUNC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_afunc_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mapplied_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNODE_UDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_nf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: apply_func() missing 1 required positional argument: 'edges'"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The second task: Link prediction\n",
    "--------------------------------\n",
    "So far, we have seen how to use DGL to implement entity classification with\n",
    "R-GCN model. In the knowledge base setting, representation generated by\n",
    "R-GCN can be further used to uncover potential relations between nodes. In\n",
    "R-GCN paper, authors feed the entity representations generated by R-GCN\n",
    "into the `DistMult <https://arxiv.org/pdf/1412.6575.pdf>`_ prediction model\n",
    "to predict possible relations.\n",
    "\n",
    "The implementation is similar to the above but with an extra DistMult layer\n",
    "stacked on top of the R-GCN layers. You may find the complete\n",
    "implementation of link prediction with R-GCN in our `example\n",
    "code <https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn/link_predict.py>`_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
