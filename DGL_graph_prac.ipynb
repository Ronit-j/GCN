{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "from dgl.contrib.data import load_data\n",
    "import numpy as np\n",
    "data = load_data(dataset='aifb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction loss is 1.656454086303711\n",
      "The prediction loss is 1.520641803741455\n",
      "The prediction loss is 1.3898983001708984\n",
      "The prediction loss is 1.2655490636825562\n",
      "The prediction loss is 1.1491563320159912\n",
      "The prediction loss is 1.0424907207489014\n",
      "The prediction loss is 0.9474363327026367\n",
      "The prediction loss is 0.8658297657966614\n",
      "The prediction loss is 0.7992105484008789\n",
      "The prediction loss is 0.7485160827636719\n",
      "The prediction loss is 0.7137940526008606\n",
      "The prediction loss is 0.6940209865570068\n",
      "The prediction loss is 0.6871294379234314\n",
      "The prediction loss is 0.6902592182159424\n",
      "The prediction loss is 0.7001688480377197\n",
      "The prediction loss is 0.7136803269386292\n",
      "The prediction loss is 0.7280322909355164\n",
      "The prediction loss is 0.7410855293273926\n",
      "The prediction loss is 0.7513815760612488\n",
      "The prediction loss is 0.758097767829895\n",
      "The prediction loss is 0.7609473466873169\n",
      "The prediction loss is 0.760063111782074\n",
      "The prediction loss is 0.7558855414390564\n",
      "The prediction loss is 0.7490631937980652\n",
      "The prediction loss is 0.7403697967529297\n",
      "The prediction loss is 0.7306305170059204\n",
      "The prediction loss is 0.7206584215164185\n",
      "The prediction loss is 0.7111960053443909\n",
      "The prediction loss is 0.7028645873069763\n",
      "The prediction loss is 0.6961216330528259\n",
      "The prediction loss is 0.6912333369255066\n",
      "The prediction loss is 0.6882631182670593\n",
      "The prediction loss is 0.687082827091217\n",
      "The prediction loss is 0.6874040961265564\n",
      "The prediction loss is 0.6888251900672913\n",
      "The prediction loss is 0.6908929347991943\n",
      "The prediction loss is 0.6931575536727905\n",
      "The prediction loss is 0.6952270269393921\n",
      "The prediction loss is 0.6968032121658325\n",
      "The prediction loss is 0.6977021098136902\n",
      "The prediction loss is 0.6978583335876465\n",
      "The prediction loss is 0.6973117589950562\n",
      "The prediction loss is 0.6961873173713684\n",
      "The prediction loss is 0.6946651935577393\n",
      "The prediction loss is 0.6929499506950378\n",
      "The prediction loss is 0.6912420392036438\n",
      "The prediction loss is 0.6897115707397461\n",
      "The prediction loss is 0.6884840130805969\n",
      "The prediction loss is 0.6876282691955566\n",
      "The prediction loss is 0.6871582865715027\n",
      "The prediction loss is 0.6870386004447937\n",
      "The prediction loss is 0.6871969103813171\n",
      "The prediction loss is 0.6875391006469727\n",
      "The prediction loss is 0.6879653334617615\n",
      "The prediction loss is 0.6883833408355713\n",
      "The prediction loss is 0.6887198090553284\n",
      "The prediction loss is 0.688926100730896\n",
      "The prediction loss is 0.6889811754226685\n",
      "The prediction loss is 0.6888895034790039\n",
      "The prediction loss is 0.6886762380599976\n",
      "The prediction loss is 0.6883803606033325\n",
      "The prediction loss is 0.6880472302436829\n",
      "The prediction loss is 0.6877207159996033\n",
      "The prediction loss is 0.6874380111694336\n",
      "The prediction loss is 0.6872242093086243\n",
      "The prediction loss is 0.6870915293693542\n",
      "The prediction loss is 0.6870384812355042\n",
      "The prediction loss is 0.687053382396698\n",
      "The prediction loss is 0.6871163249015808\n",
      "The prediction loss is 0.6872043013572693\n",
      "The prediction loss is 0.6872944831848145\n",
      "The prediction loss is 0.6873684525489807\n",
      "The prediction loss is 0.6874135732650757\n",
      "The prediction loss is 0.6874241232872009\n",
      "The prediction loss is 0.6874014139175415\n",
      "The prediction loss is 0.6873520612716675\n",
      "The prediction loss is 0.6872857213020325\n",
      "The prediction loss is 0.6872138381004333\n",
      "The prediction loss is 0.6871468424797058\n",
      "The prediction loss is 0.6870927810668945\n",
      "The prediction loss is 0.687056303024292\n",
      "The prediction loss is 0.6870386600494385\n",
      "The prediction loss is 0.6870381832122803\n",
      "The prediction loss is 0.6870499849319458\n",
      "The prediction loss is 0.687069296836853\n",
      "The prediction loss is 0.6870899200439453\n",
      "The prediction loss is 0.687107264995575\n",
      "The prediction loss is 0.6871180534362793\n",
      "The prediction loss is 0.6871204972267151\n",
      "The prediction loss is 0.6871151328086853\n",
      "The prediction loss is 0.6871035695075989\n",
      "The prediction loss is 0.6870883107185364\n",
      "The prediction loss is 0.6870718598365784\n",
      "The prediction loss is 0.6870571970939636\n",
      "The prediction loss is 0.6870458126068115\n",
      "The prediction loss is 0.6870389580726624\n",
      "The prediction loss is 0.6870366334915161\n",
      "The prediction loss is 0.6870378851890564\n",
      "The prediction loss is 0.6870418190956116\n",
      "The prediction loss is 0.6870467662811279\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is a demo for graph classification with dgl where we have a\n",
    "synthetic dataset consisting of cycle_graphs and star_graphs and\n",
    "we want to perform a binary classification.\"\"\"\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "ds = []\n",
    "for i in range(5):\n",
    "    num_nodes = 5\n",
    "    cycle = dgl.DGLGraph(nx.cycle_graph(num_nodes))\n",
    "    cycle.ndata['h'] = th.ones(num_nodes, 1)\n",
    "    ds.append((cycle, 0))\n",
    "\n",
    "    star = dgl.DGLGraph(nx.star_graph(num_nodes - 1))\n",
    "    star.ndata['h'] = th.ones(num_nodes, 1)\n",
    "    ds.append((star, 1))\n",
    "\n",
    "random.shuffle(ds)\n",
    "g_list = [data[0] for data in ds]\n",
    "labels = th.tensor([data[1] for data in ds]).float().view(-1, 1)\n",
    "\n",
    "# Model and optimizer\n",
    "weight1 = th.randn((1, 16), requires_grad=True)\n",
    "weight2 = th.randn((16, 1), requires_grad=True)\n",
    "optimizer = optim.Adam([weight1, weight2], lr=0.01)\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "# Configure message passing. With the message func and reduce func defined\n",
    "# below, the updated node feature will simply be node degree.\n",
    "msg_func = fn.copy_src(src='h', out='m')\n",
    "reduce_func = fn.sum(msg='m', out='h')\n",
    "\n",
    "# Training\n",
    "for i in range(100):\n",
    "    bg = dgl.batch(g_list)\n",
    "    # Perform message passing\n",
    "    bg.update_all(msg_func, reduce_func)\n",
    "    # Readout and get graph features for the 10 graphs.\n",
    "    bg_h = dgl.mean_nodes(bg, 'h')\n",
    "    logits = F.relu(th.mm(bg_h, weight1))\n",
    "    logits = th.mm(logits, weight2)\n",
    "    prediction = th.sigmoid(logits)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(prediction, labels)\n",
    "    print('The prediction loss is {}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
