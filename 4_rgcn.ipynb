{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Relational Graph Convolutional Network Tutorial\n",
    "================================================\n",
    "\n",
    "**Author:** Lingfan Yu, Mufei Li, Zheng Zhang\n",
    "\n",
    "The vanilla Graph Convolutional Network (GCN)\n",
    "(`paper <https://arxiv.org/pdf/1609.02907.pdf>`_,\n",
    "`DGL tutorial <http://doc.dgl.ai/tutorials/index.html>`_) exploits\n",
    "structural information of the dataset (i.e. the graph connectivity) to\n",
    "improve the extraction of node representations. Graph edges are left as\n",
    "untyped.\n",
    "\n",
    "A knowledge graph is made up by a collection of triples of the form\n",
    "(subject, relation, object). Edges thus encode important information and\n",
    "have their own embeddings to be learned. Furthermore, there may exist\n",
    "multiple edges among any given pair.\n",
    "\n",
    "A recent model Relational-GCN (R-GCN) from the paper\n",
    "`Modeling Relational Data with Graph Convolutional\n",
    "Networks <https://arxiv.org/pdf/1703.06103.pdf>`_ is one effort to\n",
    "generalize GCN to handle different relations between entities in knowledge\n",
    "base. This tutorial shows how to implement R-GCN with DGL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-GCN: a brief introduction\n",
    "---------------------------\n",
    "In *statistical relational learning* (SRL), there are two fundamental\n",
    "tasks:\n",
    "\n",
    "- **Entity classification**, i.e., assign types and categorical\n",
    "  properties to entities.\n",
    "- **Link prediction**, i.e., recover missing triples.\n",
    "\n",
    "In both cases, missing information are expected to be recovered from\n",
    "neighborhood structure of the graph. Here is the example from the R-GCN\n",
    "paper:\n",
    "\n",
    "\"Knowing that Mikhail Baryshnikov was educated at the Vaganova Academy\n",
    "implies both that Mikhail Baryshnikov should have the label person, and\n",
    "that the triple (Mikhail Baryshnikov, lived in, Russia) must belong to the\n",
    "knowledge graph.\"\n",
    "\n",
    "R-GCN solves these two problems using a common graph convolutional network\n",
    "extended with multi-edge encoding to compute embedding of the entities, but\n",
    "with different downstream processing:\n",
    "\n",
    "- Entity classification is done by attaching a softmax classifier at the\n",
    "  final embedding of an entity (node). Training is through loss of standard\n",
    "  cross-entropy.\n",
    "- Link prediction is done by reconstructing an edge with an autoencoder\n",
    "  architecture, using a parameterized score function. Training uses negative\n",
    "  sampling.\n",
    "\n",
    "This tutorial will focus on the first task to show how to generate entity\n",
    "representation. `Complete\n",
    "code <https://github.com/dmlc/dgl/tree/rgcn/examples/pytorch/rgcn>`_\n",
    "for both tasks can be found in DGL's github repository.\n",
    "\n",
    "Key ideas of R-GCN\n",
    "-------------------\n",
    "Recall that in GCN, the hidden representation for each node $i$ at\n",
    "$(l+1)^{th}$ layer is computed by:\n",
    "\n",
    "\\begin{align}h_i^{l+1} = \\sigma\\left(\\sum_{j\\in N_i}\\frac{1}{c_i} W^{(l)} h_j^{(l)}\\right)~~~~~~~~~~(1)\\\\\\end{align}\n",
    "\n",
    "where $c_i$ is a normalization constant.\n",
    "\n",
    "The key difference between R-GCN and GCN is that in R-GCN, edges can\n",
    "represent different relations. In GCN, weight $W^{(l)}$ in equation\n",
    "$(1)$ is shared by all edges in layer $l$. In contrast, in\n",
    "R-GCN, different edge types use different weights and only edges of the\n",
    "same relation type $r$ are associated with the same projection weight\n",
    "$W_r^{(l)}$.\n",
    "\n",
    "So the hidden representation of entities in $(l+1)^{th}$ layer in\n",
    "R-GCN can be formulated as the following equation:\n",
    "\n",
    "\\begin{align}h_i^{l+1} = \\sigma\\left(W_0^{(l)}h_i^{(l)}+\\sum_{r\\in R}\\sum_{j\\in N_i^r}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}\\right)~~~~~~~~~~(2)\\\\\\end{align}\n",
    "\n",
    "where $N_i^r$ denotes the set of neighbor indices of node $i$\n",
    "under relation $r\\in R$ and $c_{i,r}$ is a normalization\n",
    "constant. In entity classification, the R-GCN paper uses\n",
    "$c_{i,r}=|N_i^r|$.\n",
    "\n",
    "The problem of applying the above equation directly is rapid growth of\n",
    "number of parameters, especially with highly multi-relational data. In\n",
    "order to reduce model parameter size and prevent overfitting, the original\n",
    "paper proposes to use basis decomposition:\n",
    "\n",
    "\\begin{align}W_r^{(l)}=\\sum\\limits_{b=1}^B a_{rb}^{(l)}V_b^{(l)}~~~~~~~~~~(3)\\\\\\end{align}\n",
    "\n",
    "Therefore, the weight $W_r^{(l)}$ is a linear combination of basis\n",
    "transformation $V_b^{(l)}$ with coefficients $a_{rb}^{(l)}$.\n",
    "The number of bases $B$ is much smaller than the number of relations\n",
    "in the knowledge base.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Another weight regularization, block-decomposition, is implemented in\n",
    "   the `link prediction <link-prediction_>`_.</p></div>\n",
    "\n",
    "Implement R-GCN in DGL\n",
    "----------------------\n",
    "\n",
    "An R-GCN model is composed of several R-GCN layers. The first R-GCN layer\n",
    "also serves as input layer and takes in features (e.g. description texts)\n",
    "associated with node entity and project to hidden space. In this tutorial,\n",
    "we only use entity id as entity feature.\n",
    "\n",
    "R-GCN Layers\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "For each node, an R-GCN layer performs the following steps:\n",
    "\n",
    "- Compute outgoing message using node representation and weight matrix\n",
    "  associated with the edge type (message function)\n",
    "- Aggregate incoming messages and generate new node representations (reduce\n",
    "  and apply function)\n",
    "\n",
    "The following is the definition of an R-GCN hidden layer.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Each relation type is associated with a different weight. Therefore,\n",
    "   the full weight matrix has three dimensions: relation, input_feature,\n",
    "   output_feature.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "import dgl\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
    "                 activation=None, is_input_layer=False):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # weight bases in equation (3)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
    "                                                self.out_feat))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(self.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.bias,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases (equation (3))\n",
    "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
    "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,\n",
    "                                                        self.in_feat, self.out_feat)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        if self.is_input_layer:\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                return {'msg': embed[index] * edges.data['norm']}\n",
    "        else:\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data['rel_type']]\n",
    "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data['norm']\n",
    "                return {'msg': msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {'h': h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define full R-GCN model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        \n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        \n",
    "        # hidden to hidden\n",
    "        \n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        \n",
    "        # hidden to output\n",
    "        \n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(self.num_nodes, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu, is_input_layer=True)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.out_dim, self.num_rels, self.num_bases,\n",
    "                         activation=partial(F.softmax, dim=1))\n",
    "\n",
    "    def forward(self, batched_graph):\n",
    "        if self.features is not None:\n",
    "            batched_graph.ndata['id'] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(batched_graph)\n",
    "        return batched_graph.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset\n",
    "~~~~~~~~~~~~~~~~\n",
    "In this tutorial, we use AIFB dataset from R-GCN paper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n",
      "[[0]]\n",
      "[ 3655   941  6589  2572  4126  1153  2225  7710  3920  2666  8166  2558\n",
      "  1650  4493  6155  4488  1509  7409  4669  2011  1919  3616    76  1626\n",
      "  1069  3871   448  3962  5638  1177  1048  6810  3852  5763  5080   652\n",
      "  5791  4193  5428  7877  3236  5107  3613  3429  2246   712  2170  3795\n",
      "  5698  6995  4703  2368  1894   190  2390  6805  1535  2512  6038  1415\n",
      "   390  1061  2842  2051  3527  6724   857  2280   811  7031   737   552\n",
      "  5000  7659  1158  7765  7046  6036   876  5412  3596  1347  7850  4907\n",
      " 11971 16188 11638 11406 13658 11430 11845 16129  9852  8607 11627 14576\n",
      " 16511 14495  8914 15350 16563  9021 12752 10673  8670  9687 11405 12559\n",
      " 11059  8605 12316 10039  8574 15032 13064 14625 14765  9517 13272 16214\n",
      " 13532 10385  9124 11061 10591 11884 14823 14199 12748 15290  9066  8875\n",
      " 10247 15234 12904 11409 11534 16483 15102 14553 11940  9226 14874 10857\n",
      " 12411  9438 10510 15995 12205 10951 16451 10843  9935 12778 14440 12773\n",
      "  9794 15694 12954 10296 10204 11901  8361  9911  9354 12156  8733 12247\n",
      " 13923  9462  9333 15095 12137 14048 13365  8937 14076 12478 13713 16162\n",
      " 11521 13392 11898 11714 10531  8997 10455 12080 13983 15280 12988 10653\n",
      " 10179  8475 10675 15090  9820 10797 14323  9700  8675  9346 11127 10336\n",
      " 11812 15009  9142 10565  9096 15316  9022  8837 13285 15944  9443 16050\n",
      " 15331 14321  9161 13697 11881  9632 16135 13192]\n"
     ]
    }
   ],
   "source": [
    "# load graph data\n",
    "from dgl.contrib.data import load_data\n",
    "import numpy as np\n",
    "data = load_data(dataset='aifb')\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "train_idx = np.concatenate((train_idx,[x+num_nodes for x in train_idx]),axis=0)\n",
    "#train_idx.append(train_idx_idx+len(num_nodes))\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "print(train_idx)\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph and model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "8285\n",
      "8285\n",
      "16570\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#nx.draw(g.to_networkx(), with_labels=True)\n",
    "plt.show()\n",
    "# configurations\n",
    "n_hidden = 16 # number of hidden units\n",
    "n_bases = -1 # use number of relations as number of bases\n",
    "n_hidden_layers = 0 # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 25 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient\n",
    "#batched Graph Convolution\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.cat((labels[0],labels[1]),0)\n",
    "    print(labels)\n",
    "    return batched_graph, labels\n",
    "\n",
    "# create graph\n",
    "g = DGLGraph()\n",
    "g.add_nodes(num_nodes)\n",
    "g.add_edges(data.edge_src, data.edge_dst)\n",
    "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "\n",
    "g1 = DGLGraph()\n",
    "\n",
    "g1.add_nodes(num_nodes)\n",
    "g1.add_edges(data.edge_src, data.edge_dst)\n",
    "g1.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "list_sample = [(g,labels),(g1,labels)]\n",
    "batched_graph,labels = collate(list_sample)\n",
    "print(len(g))\n",
    "print(len(g1))\n",
    "print(len(batched_graph))\n",
    "print(isinstance(batched_graph, dgl.BatchedDGLGraph))\n",
    "# create model\n",
    "model = Model(len(batched_graph),\n",
    "              n_hidden,\n",
    "              num_classes,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronit/.conda/envs/mll/lib/python3.6/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Accuracy: 0.3036 | Train Loss: 1.3860 | Validation Accuracy: 0.3750 | Validation loss: 1.3860\n",
      "Epoch 00001 | Train Accuracy: 0.9509 | Train Loss: 1.3416 | Validation Accuracy: 0.7857 | Validation loss: 1.3599\n",
      "Epoch 00002 | Train Accuracy: 0.9509 | Train Loss: 1.2711 | Validation Accuracy: 0.8036 | Validation loss: 1.3186\n",
      "Epoch 00003 | Train Accuracy: 0.9554 | Train Loss: 1.1843 | Validation Accuracy: 0.8393 | Validation loss: 1.2651\n",
      "Epoch 00004 | Train Accuracy: 0.9598 | Train Loss: 1.0997 | Validation Accuracy: 0.8393 | Validation loss: 1.2062\n",
      "Epoch 00005 | Train Accuracy: 0.9643 | Train Loss: 1.0271 | Validation Accuracy: 0.8393 | Validation loss: 1.1489\n",
      "Epoch 00006 | Train Accuracy: 0.9688 | Train Loss: 0.9648 | Validation Accuracy: 0.8571 | Validation loss: 1.0958\n",
      "Epoch 00007 | Train Accuracy: 0.9688 | Train Loss: 0.9121 | Validation Accuracy: 0.8750 | Validation loss: 1.0475\n",
      "Epoch 00008 | Train Accuracy: 0.9688 | Train Loss: 0.8697 | Validation Accuracy: 0.8750 | Validation loss: 1.0055\n",
      "Epoch 00009 | Train Accuracy: 0.9688 | Train Loss: 0.8375 | Validation Accuracy: 0.8750 | Validation loss: 0.9710\n",
      "Epoch 00010 | Train Accuracy: 0.9688 | Train Loss: 0.8145 | Validation Accuracy: 0.8750 | Validation loss: 0.9439\n",
      "Epoch 00011 | Train Accuracy: 0.9688 | Train Loss: 0.7986 | Validation Accuracy: 0.8750 | Validation loss: 0.9236\n",
      "Epoch 00012 | Train Accuracy: 0.9688 | Train Loss: 0.7879 | Validation Accuracy: 0.8750 | Validation loss: 0.9090\n",
      "Epoch 00013 | Train Accuracy: 0.9777 | Train Loss: 0.7805 | Validation Accuracy: 0.8750 | Validation loss: 0.8986\n",
      "Epoch 00014 | Train Accuracy: 0.9821 | Train Loss: 0.7751 | Validation Accuracy: 0.8929 | Validation loss: 0.8914\n",
      "Epoch 00015 | Train Accuracy: 0.9821 | Train Loss: 0.7707 | Validation Accuracy: 0.8929 | Validation loss: 0.8861\n",
      "Epoch 00016 | Train Accuracy: 0.9821 | Train Loss: 0.7670 | Validation Accuracy: 0.8929 | Validation loss: 0.8822\n",
      "Epoch 00017 | Train Accuracy: 0.9866 | Train Loss: 0.7638 | Validation Accuracy: 0.8750 | Validation loss: 0.8791\n",
      "Epoch 00018 | Train Accuracy: 0.9866 | Train Loss: 0.7609 | Validation Accuracy: 0.8750 | Validation loss: 0.8765\n",
      "Epoch 00019 | Train Accuracy: 0.9866 | Train Loss: 0.7584 | Validation Accuracy: 0.8750 | Validation loss: 0.8743\n",
      "Epoch 00020 | Train Accuracy: 0.9866 | Train Loss: 0.7562 | Validation Accuracy: 0.8750 | Validation loss: 0.8724\n",
      "Epoch 00021 | Train Accuracy: 0.9866 | Train Loss: 0.7542 | Validation Accuracy: 0.8750 | Validation loss: 0.8708\n",
      "Epoch 00022 | Train Accuracy: 0.9955 | Train Loss: 0.7523 | Validation Accuracy: 0.8750 | Validation loss: 0.8694\n",
      "Epoch 00023 | Train Accuracy: 1.0000 | Train Loss: 0.7507 | Validation Accuracy: 0.8929 | Validation loss: 0.8682\n",
      "Epoch 00024 | Train Accuracy: 1.0000 | Train Loss: 0.7493 | Validation Accuracy: 0.8929 | Validation loss: 0.8673\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(batched_graph)\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The second task: Link prediction\n",
    "--------------------------------\n",
    "So far, we have seen how to use DGL to implement entity classification with\n",
    "R-GCN model. In the knowledge base setting, representation generated by\n",
    "R-GCN can be further used to uncover potential relations between nodes. In\n",
    "R-GCN paper, authors feed the entity representations generated by R-GCN\n",
    "into the `DistMult <https://arxiv.org/pdf/1412.6575.pdf>`_ prediction model\n",
    "to predict possible relations.\n",
    "\n",
    "The implementation is similar to the above but with an extra DistMult layer\n",
    "stacked on top of the R-GCN layers. You may find the complete\n",
    "implementation of link prediction with R-GCN in our `example\n",
    "code <https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn/link_predict.py>`_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
